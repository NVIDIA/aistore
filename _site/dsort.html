<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>DSORT | AIStore - distributed object storage</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="DSORT" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="AIStore is a lightweight object storage system with the capability to linearly scale-out with each added storage node and a special focus on petascale deep learning. See more at: github.com/NVIDIA/aistore" />
<meta property="og:description" content="AIStore is a lightweight object storage system with the capability to linearly scale-out with each added storage node and a special focus on petascale deep learning. See more at: github.com/NVIDIA/aistore" />
<link rel="canonical" href="http://localhost:4000/aistore/dsort" />
<meta property="og:url" content="http://localhost:4000/aistore/dsort" />
<meta property="og:site_name" content="AIStore - distributed object storage" />
<script type="application/ld+json">
{"headline":"DSORT","@type":"WebPage","url":"http://localhost:4000/aistore/dsort","description":"AIStore is a lightweight object storage system with the capability to linearly scale-out with each added storage node and a special focus on petascale deep learning. See more at: github.com/NVIDIA/aistore","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/aistore/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/aistore/feed.xml" title="AIStore - distributed object storage" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/aistore/">AIStore - distributed object storage</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">DSORT</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="" itemprop="datePublished">
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="dsort">DSort</h1>

<h2 id="introduction">Introduction</h2>

<p>DSort is extension for AIStore. It was designed to perform map-reduce like
operations on terabytes and petabytes of AI datasets. As a part of the whole
system, DSort is capable of taking advantage of objects stored on AIStore without
much overhead.</p>

<p>AI datasets are usually stored in tarballs, zip objects, msgpacks or tf-records.
Focusing only on these types of files and specific workload allows us to tweak
performance without too much tradeoffs.</p>

<h2 id="capabilities">Capabilities</h2>

<p>Example of map-reduce like operation which can be performed on dSort is
shuffling (in particular sorting) all objects across all shards by a given
algorithm.</p>

<p><img src="/docs/images/dsort_mapreduce.png" alt="dsort" /></p>

<p>We allow for output shards to be different size than input shards, thus a user
is also able to reshard the objects. This means that output shards can contain
more or less objects inside the shard than in the input shard, depending on
requested sizes of the shards.</p>

<p>The result of such an operation would mean that we could get output shards with
different sizes with objects that are shuffled across all the shards, which
would then be ready to be processed by a machine learning script/model.</p>

<h2 id="terms">Terms</h2>

<p><strong>Object</strong> - single piece of data. In tarballs and zip files, an <em>object</em> is
single file contained in this type of archives. In msgpack (assuming that
msgpack file is stream of dictionaries) <em>object</em> is single dictionary.</p>

<p><strong>Shard</strong> - collection of objects. In tarballs and zip files, a <em>shard</em> is whole
archive. In msgpack is the whole msgpack file.</p>

<p>We distinguish two kinds of shards: input and output. Input shards, as the name
says, it is given as an input for the dSort operation. Output on the other hand
is something that is the result of the operation. Output shards can differ from
input shards in many ways: size, number of objects, names etc.</p>

<p>Shards are assumed to be already on AIStore cluster or somewhere in the cloud bucket
so that AIStore can access them. Output shards will always be placed in the same
bucket and directory as the input shards - accessing them after completed dSort,
is the same as input shards but of course with different names.</p>

<p><strong>Record</strong> - abstracts multiple objects with same key name into single
structure. Records are inseparable which means if they come from single shard
they will also be in output shard together.</p>

<p>Eg. if we have a tarball which contains files named: <code class="highlighter-rouge">file1.txt</code>, <code class="highlighter-rouge">file1.png</code>,
<code class="highlighter-rouge">file2.png</code>, then we would have 2 <em>records</em>: one for <code class="highlighter-rouge">file1</code> and one for
<code class="highlighter-rouge">file2</code>.</p>

<p><strong>Extraction phase</strong> - dSort has multiple phases in which it does the whole
operation. The first of them is <strong>extraction</strong>. In this phase, dSort is reading
input shards and looks inside them to get to the objects and metadata. Objects
and their metadata are then extracted to either disk or memory so that dSort
won’t need another pass of the whole data set again. This way DSort can create
<strong>Records</strong> which are then used for the whole operation as the main source of
information (like location of the objects, sizes, names etc.). Extraction phase
is very critical because it does I/O operations. To make the following phases
faster, we added support for extraction to memory so that requests for the given
objects will be served from RAM instead of disk. The user can specify how much
memory can be used for the extraction phase, either in raw numbers like <code class="highlighter-rouge">1GB</code> or
percentages <code class="highlighter-rouge">60%</code>.</p>

<p>As mentioned this operation does a lot of I/O operations. To allow the user to
have better control over the disk usage, we have provided a concurrency
parameter which limits the number of shards that can be read at the same time.</p>

<p><strong>Sorting phase</strong> - in this phase, the metadata is processed and aggregated on a
single machine. It can be processed in various ways: sorting, shuffling,
resizing etc. This is usually the fastest phase but still uses a lot of CPU
processing power, to process the metadata.</p>

<p>The merging of metadata is performed in multiple steps to distribute the load
across machines.</p>

<p><strong>Creation phase</strong> - it is last phase of dSort where output shards are created.
Like the extraction phase, the creation phase is bottlenecked by disk and I/O.
Additionally, this phase may use a lot of bandwidth because objects may have
been extracted on different machine.</p>

<p>Similarly to the extraction phase we expose a concurrency parameter for the
user, to limit number of shards created simultaneously.</p>

<p>Shards are created from local records or remote records. Local records are
records which were extracted on the machine where the shard is being created,
and similarly, remote records are records which were extracted on different
machines. This means that a single machine will typically have a lot of
read/write operations on the disk coming from either local or remote requests.
This is why tweaking the concurrency parameter is really important and can have
great impact on performance. We strongly advise to make couple of tests on small
load to see what value of this parameter will result in the best performance.
Eg. tests shown that on setup: 10x targets, 10x disks on each target, the best
concurrency value is 60.</p>

<p>The other thing that user needs to remember is that when running multiple dSort
operations at once, it might be better to set the concurrency parameter to
something lower since both of the operation may use disk at the same time. A
higher concurrency parameter can result in performance degradation.</p>

<p><img src="/docs/images/dsort_shard_creation.png" alt="Shard creation" /></p>

<p><strong>Metrics</strong> - user can monitor whole operation thanks to metrics. Metrics
provide an overview of what is happening in the cluster, for example: which
phase is currently running, how much time has been spent on each phase, etc.
There are many metrics (numbers and stats) recorded for each of the phases.</p>

<h2 id="metrics">Metrics</h2>

<p>DSort allows users to fetch the statistics of a given job (either
started/running or already finished). Each phase has different, specific metrics
which can be monitored. Description of metrics returned for <em>single node</em>:</p>

<ul>
  <li><code class="highlighter-rouge">local_extraction</code>
    <ul>
      <li><code class="highlighter-rouge">started_time</code> - timestamp when the local extraction has started.</li>
      <li><code class="highlighter-rouge">end_time</code> - timestamp when the local extraction has finished.</li>
      <li><code class="highlighter-rouge">elapsed</code> - duration (in seconds) of the local extraction phase.</li>
      <li><code class="highlighter-rouge">running</code> - informs if the phase is currently running.</li>
      <li><code class="highlighter-rouge">finished</code> - informs if the phase has finished.</li>
      <li><code class="highlighter-rouge">total_count</code> - static number of shards which needs to be scanned - informs what is the expected number of input shards.</li>
      <li><code class="highlighter-rouge">extracted_count</code> - number of shards extracted/processed by given node. This number can differ from node to node since shards may not be equally distributed.</li>
      <li><code class="highlighter-rouge">extracted_size</code> - size of extracted/processed shards by given node.</li>
      <li><code class="highlighter-rouge">extracted_record_count</code> - number of records extracted (in total) from all processed shards.</li>
      <li><code class="highlighter-rouge">extracted_to_disk_count</code> - number of records extracted (in total) and saved to the disk (there was not enough space to save them in memory).</li>
      <li><code class="highlighter-rouge">extracted_to_disk_size</code> - size of extracted records which were saved to the disk.</li>
      <li><code class="highlighter-rouge">single_shard_stats</code> - statistics about single shard processing.
        <ul>
          <li><code class="highlighter-rouge">total_ms</code> - total number of milliseconds spent extracting all shards.</li>
          <li><code class="highlighter-rouge">count</code> - number of extracted shards.</li>
          <li><code class="highlighter-rouge">min_ms</code> - shortest duration of extracting a shard (in milliseconds).</li>
          <li><code class="highlighter-rouge">max_ms</code> - longest duration of extracting a shard (in milliseconds).</li>
          <li><code class="highlighter-rouge">avg_ms</code> - average duration of extracting a shard (in milliseconds).</li>
          <li><code class="highlighter-rouge">min_throughput</code> - minimum throughput of extracting a shard (in bytes per second).</li>
          <li><code class="highlighter-rouge">max_throughput</code> - maximum throughput of extracting a shard (in bytes per second).</li>
          <li><code class="highlighter-rouge">avg_throughput</code> - average throughput of extracting a shard (in bytes per second).</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><code class="highlighter-rouge">meta_sorting</code>
    <ul>
      <li><code class="highlighter-rouge">started_time</code> - timestamp when the meta sorting has started.</li>
      <li><code class="highlighter-rouge">end_time</code> - timestamp when the meta sorting has finished.</li>
      <li><code class="highlighter-rouge">elapsed</code> - duration (in seconds) of the meta sorting phase.</li>
      <li><code class="highlighter-rouge">running</code> - informs if the phase is currently running.</li>
      <li><code class="highlighter-rouge">finished</code> - informs if the phase has finished.</li>
      <li><code class="highlighter-rouge">sent_stats</code> - statistics about sending records to other nodes.
        <ul>
          <li><code class="highlighter-rouge">total_ms</code> - total number of milliseconds spent on sending the records.</li>
          <li><code class="highlighter-rouge">count</code> - number of records sent to other targets.</li>
          <li><code class="highlighter-rouge">min_ms</code> - shortest duration of sending the records (in milliseconds).</li>
          <li><code class="highlighter-rouge">max_ms</code> - longest duration of sending the records (in milliseconds).</li>
          <li><code class="highlighter-rouge">avg_ms</code> - average duration of sending the records (in milliseconds).</li>
        </ul>
      </li>
      <li><code class="highlighter-rouge">recv_stats</code> - statistics about receiving records from other nodes.
        <ul>
          <li><code class="highlighter-rouge">total_ms</code> - total number of milliseconds spent on receiving the records from nodes.</li>
          <li><code class="highlighter-rouge">count</code> - number of records received from other targets.</li>
          <li><code class="highlighter-rouge">min_ms</code> - shortest duration of receiving the records (in milliseconds).</li>
          <li><code class="highlighter-rouge">max_ms</code> - longest duration of receiving the records (in milliseconds).</li>
          <li><code class="highlighter-rouge">avg_ms</code> - average duration of receiving the records (in milliseconds).</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><code class="highlighter-rouge">shard_creation</code>
    <ul>
      <li><code class="highlighter-rouge">started_time</code> - timestamp when the shard creation has started.</li>
      <li><code class="highlighter-rouge">end_time</code> - timestamp when the shard creation has finished.</li>
      <li><code class="highlighter-rouge">elapsed</code> - duration (in seconds) of the shard creation phase.</li>
      <li><code class="highlighter-rouge">running</code> - informs if the phase is currently running.</li>
      <li><code class="highlighter-rouge">finished</code> - informs if the phase has finished.</li>
      <li><code class="highlighter-rouge">to_create</code> - number of shards which needs to be created on given node.</li>
      <li><code class="highlighter-rouge">created_count</code> - number of shards already created.</li>
      <li><code class="highlighter-rouge">moved_shard_count</code> - number of shards moved from the node to another one (it sometimes makes sense to create shards locally and send it via network).</li>
      <li><code class="highlighter-rouge">req_stats</code> - statistics about sending requests for records.
        <ul>
          <li><code class="highlighter-rouge">total_ms</code> - total number of milliseconds spent on sending requests for records from other nodes.</li>
          <li><code class="highlighter-rouge">count</code> - number of requested records.</li>
          <li><code class="highlighter-rouge">min_ms</code> - shortest duration of sending a request (in milliseconds).</li>
          <li><code class="highlighter-rouge">max_ms</code> - longest duration of sending a request (in milliseconds).</li>
          <li><code class="highlighter-rouge">avg_ms</code> - average duration of sending a request (in milliseconds).</li>
        </ul>
      </li>
      <li><code class="highlighter-rouge">resp_stats</code> - statistics about waiting for the records.
        <ul>
          <li><code class="highlighter-rouge">total_ms</code> - total number of milliseconds spent on waiting for the records from other nodes.</li>
          <li><code class="highlighter-rouge">count</code> - number of records received from other nodes.</li>
          <li><code class="highlighter-rouge">min_ms</code> - shortest duration of waiting for a record (in milliseconds).</li>
          <li><code class="highlighter-rouge">max_ms</code> - longest duration of waiting for a record (in milliseconds).</li>
          <li><code class="highlighter-rouge">avg_ms</code> - average duration of waiting for a record (in milliseconds).</li>
        </ul>
      </li>
      <li><code class="highlighter-rouge">local_send_stats</code> - statistics about sending record content to other target.
        <ul>
          <li><code class="highlighter-rouge">total_ms</code> - total number of milliseconds spent on writing the record content to the wire.</li>
          <li><code class="highlighter-rouge">count</code> - number of records received from other nodes.</li>
          <li><code class="highlighter-rouge">min_ms</code> - shortest duration of waiting for a record content to written into the wire (in milliseconds).</li>
          <li><code class="highlighter-rouge">max_ms</code> - longest duration of waiting for a record content to written into the wire (in milliseconds).</li>
          <li><code class="highlighter-rouge">avg_ms</code> - average duration of waiting for a record content to written into the wire (in milliseconds).</li>
          <li><code class="highlighter-rouge">min_throughput</code> - minimum throughput of writing record content into the wire (in bytes per second).</li>
          <li><code class="highlighter-rouge">max_throughput</code> - maximum throughput of writing record content into the wire (in bytes per second).</li>
          <li><code class="highlighter-rouge">avg_throughput</code> - average throughput of writing record content into the wire (in bytes per second).</li>
        </ul>
      </li>
      <li><code class="highlighter-rouge">local_recv_stats</code> - statistics receiving record content from other target.
        <ul>
          <li><code class="highlighter-rouge">total_ms</code> - total number of milliseconds spent on receiving the record content from the wire.</li>
          <li><code class="highlighter-rouge">count</code> - number of records received from other nodes.</li>
          <li><code class="highlighter-rouge">min_ms</code> - shortest duration of waiting for a record content to be read from the wire (in milliseconds).</li>
          <li><code class="highlighter-rouge">max_ms</code> - longest duration of waiting for a record content to be read from the wire (in milliseconds).</li>
          <li><code class="highlighter-rouge">avg_ms</code> - average duration of waiting for a record content to be read from the wire (in milliseconds).</li>
          <li><code class="highlighter-rouge">min_throughput</code> - minimum throughput of reading record content from the wire (in bytes per second).</li>
          <li><code class="highlighter-rouge">max_throughput</code> - maximum throughput of reading record content from the wire (in bytes per second).</li>
          <li><code class="highlighter-rouge">avg_throughput</code> - average throughput of reading record content from the wire (in bytes per second).</li>
        </ul>
      </li>
      <li><code class="highlighter-rouge">single_shard_stats</code> - statistics about single shard creation.
        <ul>
          <li><code class="highlighter-rouge">total_ms</code> - total number of milliseconds spent creating all shards.</li>
          <li><code class="highlighter-rouge">count</code> - number of created shards.</li>
          <li><code class="highlighter-rouge">min_ms</code> - shortest duration of creating a shard (in milliseconds).</li>
          <li><code class="highlighter-rouge">max_ms</code> - longest duration of creating a shard (in milliseconds).</li>
          <li><code class="highlighter-rouge">avg_ms</code> - average duration of creating a shard (in milliseconds).</li>
          <li><code class="highlighter-rouge">min_throughput</code> - minimum throughput of creating a shard (in bytes per second).</li>
          <li><code class="highlighter-rouge">max_throughput</code> - maximum throughput of creating a shard (in bytes per second).</li>
          <li><code class="highlighter-rouge">avg_throughput</code> - average throughput of creating a shard (in bytes per second).</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><code class="highlighter-rouge">aborted</code> - informs if the job has been aborted.</li>
  <li><code class="highlighter-rouge">archived</code> - informs if the job has finished and was archived to journal.</li>
  <li><code class="highlighter-rouge">description</code> - description of the job.</li>
</ul>

<p>Example output for single node:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
  "local_extraction": {
    "started_time": "2019-06-17T12:27:25.102691781+02:00",
    "end_time": "2019-06-17T12:28:04.982017787+02:00",
    "elapsed": 39,
    "running": false,
    "finished": true,
    "total_count": 1000,
    "extracted_count": 182,
    "extracted_size": 4771020800,
    "extracted_record_count": 9100,
    "extracted_to_disk_count": 4,
    "extracted_to_disk_size": 104857600,
    "single_shard_stats": {
      "total_ms": 251417,
      "count": 182,
      "min_ms": 30,
      "max_ms": 2696,
      "avg_ms": 1381,
      "min_throughput": 9721724,
      "max_throughput": 847903603,
      "avg_throughput": 50169799
    }
  },
  "meta_sorting": {
    "started_time": "2019-06-17T12:28:04.982041542+02:00",
    "end_time": "2019-06-17T12:28:05.336979995+02:00",
    "elapsed": 0,
    "running": false,
    "finished": true,
    "sent_stats": {
      "total_ms": 99,
      "count": 1,
      "min_ms": 99,
      "max_ms": 99,
      "avg_ms": 99
    },
    "recv_stats": {
      "total_ms": 246,
      "count": 1,
      "min_ms": 246,
      "max_ms": 246,
      "avg_ms": 246
    }
  },
  "shard_creation": {
    "started_time": "2019-06-17T12:28:05.725630555+02:00",
    "end_time": "2019-06-17T12:29:19.108651924+02:00",
    "elapsed": 73,
    "running": false,
    "finished": true,
    "to_create": 9988,
    "created_count": 9988,
    "moved_shard_count": 0,
    "req_stats": {
      "total_ms": 160,
      "count": 8190,
      "min_ms": 0,
      "max_ms": 20,
      "avg_ms": 0
    },
    "resp_stats": {
      "total_ms": 4323665,
      "count": 8190,
      "min_ms": 0,
      "max_ms": 6829,
      "avg_ms": 527
    },
    "single_shard_stats": {
      "total_ms": 4487385,
      "count": 9988,
      "min_ms": 0,
      "max_ms": 6829,
      "avg_ms": 449,
      "min_throughput": 76989,
      "max_throughput": 709852568,
      "avg_throughput": 98584381
    }
  },
  "aborted": false,
  "archived": true
}
</code></pre></div></div>

<h2 id="api">API</h2>

<p>You can use the <a href="/cli/README.md">AIS’s CLI</a> to start, abort, retrieve metrics or list dSort jobs.
It is also possible generate random dataset to test dSort’s capabilities.</p>

<h2 id="config">Config</h2>

<table>
  <thead>
    <tr>
      <th>Config value</th>
      <th>Default value</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="highlighter-rouge">duplicated_records</code></td>
      <td>“ignore”</td>
      <td>what to do when duplicated records are found: “ignore” - ignore and continue, “warn” - notify a user and continue, “abort” - abort dSort operation</td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">missing_shards</code></td>
      <td>“ignore”</td>
      <td>what to do when missing shards are detected: “ignore” - ignore and continue, “warn” - notify a user and continue, “abort” - abort dSort operation</td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">ekm_malformed_line</code></td>
      <td>“abort”</td>
      <td>what to do when extraction key map notices a malformed line: “ignore” - ignore and continue, “warn” - notify a user and continue, “abort” - abort dSort operation</td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">ekm_missing_key</code></td>
      <td>“abort”</td>
      <td>what to do when extraction key map have a missing key: “ignore” - ignore and continue, “warn” - notify a user and continue, “abort” - abort dSort operation</td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">call_timeout</code></td>
      <td>“10m”</td>
      <td>a maximum time a target waits for another target to respond</td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">default_max_mem_usage</code></td>
      <td>“80%”</td>
      <td>a maximum amount of memory used by running dSort. Can be set as a percent of total memory(e.g <code class="highlighter-rouge">80%</code>) or as the number of bytes(e.g, <code class="highlighter-rouge">12G</code>)</td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">dsorter_mem_threshold</code></td>
      <td>“100GB”</td>
      <td>minimum free memory threshold which will activate specialized dsorter type which uses memory in creation phase - benchmarks shows that this type of dsorter behaves better than general type</td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">compression</code></td>
      <td>“never”</td>
      <td>LZ4 compression parameters used when dSort sends its shards over network. Values: “never” - disables, “always” - compress all data, or a set of rules for LZ4, e.g “ratio=1.2” means enable compression from the start but disable when average compression ratio drops below 1.2 to save CPU resources</td>
    </tr>
  </tbody>
</table>

<p>To clear what these values means we have couple examples to showcase certain scenarios.</p>

<h3 id="examples">Examples</h3>

<h4 id="default_max_mem_usage"><code class="highlighter-rouge">default_max_mem_usage</code></h4>

<p>Lets assume that we have <code class="highlighter-rouge">N</code> targets, where each target has <code class="highlighter-rouge">Y</code>GB of RAM and <code class="highlighter-rouge">default_max_mem_usage</code> is set to <code class="highlighter-rouge">80%</code>.
So dSort can allocate memory until the number of memory used (total in the system) is below <code class="highlighter-rouge">80% * Y</code>GB.
What this means is that regardless of how much other subsystems or programs working at the same instance use memory, the dSort will never allocate memory if the watermark is reached.
For example if some other program already allocated <code class="highlighter-rouge">90% * Y</code>GB memory (only <code class="highlighter-rouge">10%</code> is left), then dSort will not allocate any memory since it will notice that the watermark is already exceeded.</p>

<h4 id="dsorter_mem_threshold"><code class="highlighter-rouge">dsorter_mem_threshold</code></h4>

<p>DSort has implemented for now 2 different types of so called “dsorter”: <code class="highlighter-rouge">dsorter_mem</code> and <code class="highlighter-rouge">dsorter_general</code>.
These two implementations use memory, disks and network a little bit differently and are designated to different use cases.</p>

<p>By default <code class="highlighter-rouge">dsorter_general</code> is used as it was implemented for all types of workloads.
It is allocates memory during the first phase of dSort and uses it in the last phase.</p>

<p><code class="highlighter-rouge">dsorter_mem</code> was implemented in mind to speed up the creation phase which is usually a biggest bottleneck.
It has specific way of building shards in memory and then persisting them do the disk.
This makes this dsorter memory oriented and it is required for it to have enough memory to build the shards.</p>

<p>To determine which dsorter to use we have introduced a heuristic which tries to determine when it is best to use <code class="highlighter-rouge">dsorter_mem</code> instead of <code class="highlighter-rouge">dsorter_general</code>.
Config value <code class="highlighter-rouge">dsorter_mem_threshold</code> sets the threshold above which the <code class="highlighter-rouge">dsorter_mem</code> will be used.
If <strong>all</strong> targets have max memory usage (see <code class="highlighter-rouge">default_max_mem_usage</code>) above the <code class="highlighter-rouge">dsorter_mem_threshold</code> then <code class="highlighter-rouge">dsorter_mem</code> is chosen for the dSort job.
For example if each target has <code class="highlighter-rouge">Y</code>GB of RAM, <code class="highlighter-rouge">default_max_mem_usage</code> is set to <code class="highlighter-rouge">80%</code> and <code class="highlighter-rouge">dsorter_mem_threshold</code> is set to <code class="highlighter-rouge">100GB</code> then as long as on all targets <code class="highlighter-rouge">80% * Y &gt; 100GB</code> then <code class="highlighter-rouge">dsorter_mem</code> will be used.</p>

  </div><a class="u-url" href="/aistore/dsort" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/aistore/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">AIStore - distributed object storage</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">AIStore - distributed object storage</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>AIStore is a lightweight object storage system with the capability to linearly scale-out with each added storage node and a special focus on petascale deep learning. See more at: github.com/NVIDIA/aistore
</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
