<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>DEPLOY/PROD/K8S | AIStore - distributed object storage</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="DEPLOY/PROD/K8S" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="AIStore is a lightweight object storage system with the capability to linearly scale-out with each added storage node and a special focus on petascale deep learning. See more at: github.com/NVIDIA/aistore" />
<meta property="og:description" content="AIStore is a lightweight object storage system with the capability to linearly scale-out with each added storage node and a special focus on petascale deep learning. See more at: github.com/NVIDIA/aistore" />
<link rel="canonical" href="http://localhost:4000/aistore/deploy/prod/k8s" />
<meta property="og:url" content="http://localhost:4000/aistore/deploy/prod/k8s" />
<meta property="og:site_name" content="AIStore - distributed object storage" />
<script type="application/ld+json">
{"headline":"DEPLOY/PROD/K8S","@type":"WebPage","url":"http://localhost:4000/aistore/deploy/prod/k8s","description":"AIStore is a lightweight object storage system with the capability to linearly scale-out with each added storage node and a special focus on petascale deep learning. See more at: github.com/NVIDIA/aistore","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/aistore/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/aistore/feed.xml" title="AIStore - distributed object storage" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/aistore/">AIStore - distributed object storage</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">DEPLOY/PROD/K8S</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="" itemprop="datePublished">
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="deploying-ais-on-k8s">Deploying AIS on k8s</h1>

<h2 id="introduction">Introduction</h2>

<p>This document will detail the steps required to deploy AIS on k8s. It will assume a blank slate and detail all steps. If you already have a k8s installation on which you want to deploy AIS many of these steps can be skipped, but some are still essential. Each section will begin with an indication of whether its steps are required or optional, and what assumptions are made.</p>

<p>As a high-performance and (of course) stateful application, AIS has more moving parts and demands than your average standalone containerized application - but they’re not difficult to realize on the k8s platform.</p>

<h3 id="conventions-in-this-document">Conventions in this Document</h3>

<h4 id="tl-dr">TL; DR</h4>

<p>Steps that you are likely/required to perform are highlighted like this. The intention is that you could step only over those highlighted bits of text and use the other text to fill in detail as needed.</p>

<h2 id="high-level-overview">High Level Overview</h2>

<h3 id="deployment-type">Deployment Type</h3>

<p>AIS is a high performance storage application for DL applications. It can be deployed as a storage backend, as a caching level between storage backends (including cloud) and clients, or as a combination. In this document we detail deployment as a storage backend.</p>

<h3 id="ais-terminology">AIS Terminology</h3>

<p>An AIS deployment consists of a number of <em>target pods</em> and <em>gateway pods</em>, all cooperating within an <em>AIS cluster instance</em>. The target pods are deployed with persistent storage from the nodes on which they run - one target per node. The gateway pods together combine to offer a k8s service that external DL clients contact (using http) to read/write data - the gateway service redirects the client to the correct target endpoint to service the request.</p>

<h3 id="prerequisites">Prerequisites</h3>

<ol>
  <li>[Hard] <strong>Bare-metal Kubernetes with Calico networking</strong>. To achieve the levels of performance required to service DL applications, AIS expects to be deployed on baremetal k8s - i.e., no hypervisor running underneath k8s. For experimentation it is possible to deploy in VMs, but for production use bare-metal is a hard requirement. K8s 1.14.1 or later is expected (tested). We only support Docker container runtime. The Calico CNI plugin has proven to meet the performance requirements of AIS.</li>
  <li>[Hard] <strong>Privileged AIS container</strong> - the AIS container must be run with privilege within k8s.</li>
  <li>[Hard] **External DNS/IP assigned **for clients to contact the AIS gateway service; external meaning “outside the k8s cluster, in DC”. We also support DGX GPU clients within the cluster - but assuming there are to be external clients they need a stable IP to contact.</li>
  <li>[Hard] <strong>Access to ports 51080 and 51081</strong>. The AIS gateway service is an ingress on port 51080, and redirects clients to target pods via hostport 51081 on target nodes.</li>
  <li>[Hard] <strong>Kubernetes nodes with persistent storage</strong> in local filesystems to be dedicated to AIS. We assume HDD since AIS is intended to achieve high DL performance without requiring SSD/NVME. Our reference configuration has 10 HDD in each of 12 target nodes in a single rack, but smaller or larger deployments are not simpler or more-complex! Single node deployment is possible for experimentation.</li>
  <li>[Strong]** Identical storage present on each target node** - more of a limitation of existing deployment scripts than a requirement of AIS</li>
  <li>[Very strong] <strong>Ubuntu Linux, 18.04</strong> - that is all we have tested on.</li>
  <li>[Very strong] <strong>XFS local filesystems</strong> for persistent storage in AIS. That is what we’ve tested with, and what the provided playbooks will create.</li>
  <li>[Obvious!] <strong>High-performance networking</strong> both in target node NICs and in switches. Our reference configuration has used 50GigE for storage nodes and 100GigE on DGX GPU clients. We would <strong>strongly recommend</strong> a 100GE link to each storage server as well!</li>
  <li>[Recommended] <strong>Ansible</strong> - we provide a number of playbooks for host configuration and AIS preparation; if you don’t use them you’ll have to tease apart their steps and perform them manually!</li>
  <li>[Recommended] <strong>Grafana</strong> - the default deployment will install Grafana, Graphite and Prometheus in the k8s cluster for performance monitoring. Some storage is required to persist the monitoring data.</li>
  <li>[Recommended] <strong>Dedicated k8s Cluster</strong> - You <em>can</em> deploy within a big established cluster on just a small subset of nodes, but we’ve tested on standalone clusters so recommend deploying as such. For example some playbooks still target the whole cluster by default.</li>
</ol>

<h3 id="planning">Planning</h3>

<p>It’s helpful to note the planned configuration before embarking on a deployment. Complete a table along the lines of the following (filled with our reference configuration). AIS target and gateway pods are deployed via k8s DaemonSets on nodes selected by node labeling - maximum one each of target/gateway pods per node.</p>

<table>
  <tbody>
    <tr>
      <td><strong>Item</strong></td>
      <td><strong>Resources</strong></td>
      <td><strong>Details</strong></td>
    </tr>
    <tr>
      <td><strong>Target nodes</strong></td>
      <td>cpu01 - cpu12</td>
      <td>10 x 10TB HDD: sda, sdb, …, sdj all XFS mounted at /ais/sd[a-j] ~1.2PB total</td>
    </tr>
    <tr>
      <td><strong>Gateway pods</strong></td>
      <td>cpu01 - cpu12</td>
      <td>HA service, easiest to schedule one gateway pod per target pod/node</td>
    </tr>
    <tr>
      <td><strong>Initial gateway</strong></td>
      <td>cpu01</td>
      <td>Deployment quirk! Must be one of the gateway pod nodes.</td>
    </tr>
    <tr>
      <td><strong>Graphite</strong></td>
      <td>cpu01</td>
      <td>/data/graphite, 250Gi</td>
    </tr>
    <tr>
      <td><strong>Grafana</strong></td>
      <td>cpu01</td>
      <td>/data/grafana, 250Gi</td>
    </tr>
  </tbody>
</table>

<h2 id="host-preparation">Host Preparation</h2>

<h3 id="host-os">Host OS</h3>

<h4 id="installation-recommended">Installation [Recommended]</h4>

<p>[Recommended/Tested] Install Ubuntu 18.04 on all planned target nodes (e.g., we use Foreman to perform this step along with a few of the following steps).</p>

<h4 id="ansible-inventory-recommended">Ansible Inventory [Recommended]</h4>

<p>Make sure that Ansible has passwordless sudo access to all storage node (i.e., ssh keys for login without password, no password required for sudo.</p>

<p>Since we use Kubespray to deploy k8s, our playbooks use Kubespray host group names by default (can over-ride on cmdline). It is convenient to add an inventory group covering all intended target nodes: we call that group <code class="highlighter-rouge">cpu-worker-node</code> below (please create it - it is used in some playbooks).</p>

<h4 id="playbook-1-host-configuration-required">Playbook 1: Host Configuration [Required]</h4>

<p>Once hosts are installed they <em>must</em> be configured for AIS using the provided Ansible playbook. Before running the playbook, check the values in <code class="highlighter-rouge">vars.yaml</code>. This playbook runs against the whole cluster by default, limit with -e playhosts=... if needed.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    $ cd $AISSRC/deploy/prod/k8s/playbooks
    $ ansible-playbook -i $INVENTORY ./ais_host_config_common.yml --become
</code></pre></div></div>

<p>(assuming group target-nodes is in your inventory). This playbook:</p>

<ul>
  <li>Disables unattended upgrades in Ubuntu</li>
  <li>Installs a long list of packages (see <code class="highlighter-rouge">vars.yaml</code>); many/all of these are not essential for k8s or AIS in a containerized deployment - they’re installed for debug and monitoring</li>
  <li>Configures ulimits and performs system tuning via <code class="highlighter-rouge">/etc/sysctl.d</code></li>
  <li>Configures a networking MTU of 9000 for all interfaces listed in the <code class="highlighter-rouge">ais_host_mtu</code> list of <code class="highlighter-rouge">vars.yaml</code>; the standard entry is for NIC <code class="highlighter-rouge">enp94s0</code> driver <code class="highlighter-rouge">mlx5_core</code> - adjust for your site.</li>
  <li>Selects the performance governor for CPU frequency management</li>
  <li>Creates an <code class="highlighter-rouge">ais_host_config</code> systemctl unit to tune things that can’t be done from sysctl.d:
    <ul>
      <li>Applies block device tuning as per <code class="highlighter-rouge">vars.yaml</code> (see <code class="highlighter-rouge">blkdevtune</code> there)</li>
      <li>Perform some Mellanox ethtool tuning for ring params and number of channels; this can be controlled from <code class="highlighter-rouge">vars.yaml </code>(see <code class="highlighter-rouge">ethtool</code> section there)</li>
    </ul>
  </li>
</ul>

<h4 id="playbook-2-enable-multiqueue-io-and-reboot">Playbook 2: Enable Multiqueue IO and Reboot</h4>

<p>To change IO scheduler run this playbook (runs against cpu-worker-node by default):</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    $ cd $AISSRC/deploy/prod/k8s/playbooks
    $ ansible-playbook -i $INVENTORY ./ais_enable_multiqueue.yml --become 
</code></pre></div></div>

<p>This playbook modifies the GRUB configuration. You must reboot hosts after running it.</p>

<h4 id="playbook-3-ntp">Playbook 3: NTP</h4>

<p>If NTP in your datacenter is required to use a local pool server then list the pool servers in vars.yaml and run this playbook.</p>

<h4 id="playbook-4-make-data-filesystems">Playbook 4: Make Data Filesystems</h4>

<p>This playbook performs <code class="highlighter-rouge">mkfs</code> for all filesystems and mounts them - use with care! You are required to list target nodes and their disks on the cmdline (assumes same disk config for all).</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ `cd $AISSRC/deploy/prod/k8s/playbooks`
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    $ ansible-playbook -i $INVENTORY ./ais_datafs_mkfs.yaml \
      -e '{"ais_hosts": ["cpu01", "cpu02", "cpu03", "cpu04", "cpu05", \
         "cpu06", "cpu07", "cpu08", "cpu09", "cpu10", "cpu11", "cpu12" ], \
         "ais_devices": ["sda", "sdb", "sdc", "sdd", "sde", "sdf", \
                         "sdg", "sdh", "sdi", "sdj"]}' \
      --become
</code></pre></div></div>

<p>Yes it could do with some ease-of-use improvements, but you should not need it often!</p>

<p>Note that we use XFS filesystems with specific mount options, and the playbooks mount <code class="highlighter-rouge">sd*</code> at <code class="highlighter-rouge">/ais/sd*</code> on the target node hosts.</p>

<h3 id="kubespray">Kubespray</h3>

<p>Before building an initial k8s cluster, or when adding nodes, easiest is to run the above host configuration steps before building/scaling the cluster, but it is not essential.</p>

<p>You can build the k8s cluster with other solutions - Kubespray has simply worked nicely for our needs.</p>

<p>We clone <a href="https://github.com/kubernetes-sigs/kubespray">https://github.com/kubernetes-sigs/kubespray</a>, copy the sample inventory into a parallel tree, and apply the following edits (you will likely have to tweak values - essential ones are highlighted):</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    $ diff -r sample/ aiscluster/
    diff -r sample/group_vars/all/docker.yml aiscluster/group_vars/all/docker.yml
    51a52,53
    &gt; docker_version: 18.09
    &gt; 
    diff -r sample/group_vars/k8s-cluster/addons.yml aiscluster/group_vars/k8s-cluster/addons.yml
    7c7
    &lt; helm_enabled: false
    ---
    &gt; helm_enabled: true
    10c10
    &lt; registry_enabled: false
    ---
    &gt; registry_enabled: true
    16c16
    &lt; metrics_server_enabled: false
    ---
    &gt; metrics_server_enabled: true
    diff -r sample/group_vars/k8s-cluster/k8s-cluster.yml aiscluster/group_vars/k8s-cluster/k8s-cluster.yml
    23c23
    &lt; kube_version: v1.14.3
    ---
    &gt; kube_version: v1.14.1
    81c81,82
    &lt; kube_service_addresses: 10.233.0.0/18
    ---
    &gt; #kube_service_addresses: 10.233.0.0/18
    &gt; kube_service_addresses: 192.168.0.0/18
    86c87,88
    &lt; kube_pods_subnet: 10.233.64.0/18
    ---
    &gt; #kube_pods_subnet: 10.233.64.0/18
    &gt; kube_pods_subnet: 192.168.64.0/18
    127c129
    &lt; cluster_name: cluster.local
    ---
    &gt; cluster_name: aiscluster.local
    136a139,140
    &gt; # for fix of https://github.com/kubernetes/dns/issues/292
    &gt; nodelocaldns_version: "1.15.2"
    178c182
    &lt; # kubeconfig_localhost: false
    ---
    &gt; kubeconfig_localhost: true
    180c184
    &lt; # kubectl_localhost: false
    ---
    &gt; kubectl_localhost: true
    diff -r sample/group_vars/k8s-cluster/k8s-net-calico.yml aiscluster/group_vars/k8s-cluster/k8s-net-calico.yml
    9c9
    &lt; # nat_outgoing: true
    ---
    &gt; nat_outgoing: true
    23c23
    &lt; # calico_mtu: 1500
    ---
    &gt; calico_mtu: 8980
</code></pre></div></div>

<p>The only key value there is calico_mtu - this must be at least 20 bytes less than the host NIC MTU (which we set to 9000).</p>

<p>With the new inventory tweaked as above, build the k8s cluster:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ~/kubespray$ ansible-playbook -i inventory/aiscluster/hosts.ini cluster.yml --become
</code></pre></div></div>

<p>Note: the Ansible inventory used <span style="text-decoration:underline;">must</span> be part of the kubespray tree as above (with group_vars etc as copied from sample) - if you point to a standalone inventory outside the tree then Kubespray has all sorts of subtle failures!</p>

<p>We assume hereafter that the k8s cluster is configured and that you’re able to run kubectl, helm etc as needed.</p>

<h4 id="playbook-5-post-kubespray-configuration">Playbook 5: Post-Kubespray Configuration</h4>

<p>After creating a k8s cluster (through whatever means) or after adding a node to an existing cluster, if you intend to run an AIS pod on that node then you must run the following:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    $ cd $AISSRC/deploy/prod/k8s/playbooks
    $ ansible-playbook -i $INVENTORY ./ais_host_post_kubespray.yml --become 
</code></pre></div></div>

<p>AIS will not operate without this step. The playbook (which targets the whole cluster by default, be more selective with <code class="highlighter-rouge">-e playhosts=...</code>) adds the non-default option <code class="highlighter-rouge">--allowed-unsafe-sysctls='net.core.somaxconn'</code> to kubelet  config file <code class="highlighter-rouge">/etc/kubernetes/kubelet.env</code> and restarts kubelet on play hosts.</p>

<h2 id="kubernetes-preparation">Kubernetes Preparation</h2>

<p>At this stage we have an operational k8s cluster with candidate AIS target nodes configured as required and with the relevant local XFS filesystem made ready for AIS to populate. A little more preparation is needed before deploying the AIS application.</p>

<h3 id="node-labeling">Node Labeling</h3>

<p>The target and gateway pods are created in DaemonSets, controlled by k8s node labeling. You need to label:</p>
<ul>
  <li>All designated gateway nodes (ie k8s nodes to host gateway pods, one per node) with <code class="highlighter-rouge">nvidia.com/ais-proxy=&lt;release&gt;-electable</code></li>
  <li>Exactly one, it doesn’t matter which, of the gateway nodes must also be labeled <code class="highlighter-rouge">nvidia.com/ais-initial-primary-proxy=&lt;release&gt;</code></li>
  <li>All designated target nodes must be labeled <code class="highlighter-rouge">nvidia.com/ais-target=&lt;release&gt;</code>
in which       <release> is the name of the Helm release you use in `helm install --name=...`</release>
  </li>
</ul>

<p>There’s no need to start in the final intended configuration, for example you can start a cluster with just one proxy (which will have to be that labeled initial primary) and a single target node and label additional nodes as required. You can label nodes before Helm install or after.</p>

<p>There’s no playbook for labeling, so use shell such as:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    $ kubectl label node cpu01 nvidia.com/ais-initial-primary-proxy=demo
    $ for ((i=1; i&lt;=12; i=i+1 )); do
    	nodename=$(printf "cpu%02d" $i)
    	kubectl label node $nodename nvidia.com/ais-proxy=demo-electable
    	kubectl label node $nodename ais-target=demo
    done
</code></pre></div></div>

<p>for the case of using ‘demo’ as the Helm release name (i.e., <code class="highlighter-rouge">helm install --name=demo ...</code>)</p>

<h3 id="storage-for-ais">Storage For AIS</h3>

<p>We’ve already made XFS filesystems in host preparation above. The k8s deployments of the target pods access those filesystems via k8s hostPath volumes - we’ll simply pass the list of disks for each node on the helm install cmdline (via the wrapper created below).</p>

<p>The playbooks used above mounted <code class="highlighter-rouge">sda</code> at <code class="highlighter-rouge">/ais/sda</code>, and so on. Our reference configuration, in which each target node has 10 HDD <code class="highlighter-rouge">sda-sdj</code> will pass the corresponding 10 mountpoints.</p>

<h3 id="storage-for-grafanagraphite">Storage For Grafana/Graphite</h3>

<p>The default AIS helm chart expects to deploy Grafana/Graphite/Prometheus within the cluster - the installation can be suppressed if another solution is already in place. This deployment is automated, but in advance you need to identify some persistent storage for Grafana and Graphite. This can be done with any volume type, but the AIS chart is expecting the simplest hostPath approach in which you just need to identify the node and path for the storage (having created the directory). Avoid using any of the disks on which AIS data is also stored.</p>

<p>For example, assume there’s some disk space available on node cpu01 - in this example we had 2 x 1TB nvme drives which were joined into a single 2TB lvm volume mounted at /data; to prepare for AIS just</p>

<p>cpu01$ mkdir /data/grafana /data/graphite</p>

<p>Note the node name and paths for use in the start wrapper below.</p>

<h3 id="external-ingress">External Ingress</h3>

<h4 id="ais-gateway-external-ip">AIS Gateway External IP</h4>

<p>First, assign a datacenter IP that clients will contact for AIS services. Only one is required, for the AIS gateway service (redirections to target pods will be via hostPorts of their respective nodes). We assume the IP is already suitably routed.</p>

<h4 id="loadbalancer">LoadBalancer</h4>

<p>We use metalLB as a loadbalancer for external client ingress to the k8s cluster. If other load balancer solutions are available (e.g. provided by cloud vendor) they could be used instead.</p>

<p>The simplest metalLB setup is to use a layer 2 solution in which one k8s node responds to the MAC address associated with the external IP, and if that host goes awol another is promoted to take over. The alternative is to use BGP, which has some complications when also using Calico networking. So the following assumes the layer 2 solution.</p>

<p>Begin by installing metalLB, as per <a href="https://metallb.universe.tf/installation/">https://metallb.universe.tf/installation/</a> :</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    $ helm install -n metallb --namespace metallb-system stable/metallb
</code></pre></div></div>

<p>Next,  configure metalLB by editing (in namespace metallb-system) configmap/metallb-config. We have provided a sample layer 2 configuration which can be used with <code class="highlighter-rouge">kubectl apply -f</code>:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    $ cat deploy/prod/k8s/helm/ais/mlbcm.yaml
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: metallb-config
      namespace: metallb-system
    data:
      config: |
        address-pools:
        - name: default
          protocol: layer2
          addresses:
          - 10.132.179.10-10.132.179.20
</code></pre></div></div>

<p>Note: that example and the service created in ais install assume use of namespace metallb-system in installing metalLB.</p>

<p>The address pool range specified will be controlled/allocated by metalLB - it should contain the external IP chosen for the AIS gateway service.</p>

<p>When AIS is installed with Helm, a service will be created requesting the external IP chosen here.</p>

<h4 id="required-open-ports">Required Open Ports</h4>

<p>You may need to open firewall rules to AIS traffic. AIS uses HTTP, but not on the usual http ports. It uses (by default) port 51080 for the gateway service, and 51081 for target pods. It is possible to configure different host port numbers to pod port numbers - but it’s less confusing to keep them the same.</p>

<p>Port 51080 (or other value if you override below) must be open on all k8s nodes that run proxy pods. The externalIP will only resolve to one of the nodes at a time, but it can move around over time.</p>

<p>Port 51081 must be open on all k8s nodes that run target pods. Client requests to the gateway service are redirected to hostPort 51081 on the target node (and from there into the target pod it hosts).</p>

<h3 id="ais-containers">AIS Containers</h3>

<p>Containers are currently hosted on quay.io in private repos.</p>

<table>
  <tbody>
    <tr>
      <td><strong>Repo</strong></td>
      <td><strong>Version</strong></td>
      <td><strong>Description</strong></td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">quay.io/nvidia/aisnode</code></td>
      <td><code class="highlighter-rouge">2.5</code></td>
      <td>AIS container for target and proxy nodes</td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">quay.io/nvidia/ais-kubectl</code></td>
      <td><code class="highlighter-rouge">stable</code></td>
      <td>Used briefly in an initContainer to label pods, hardly ever changes</td>
    </tr>
  </tbody>
</table>

<h3 id="ais-administrative-cli">AIS Administrative CLI</h3>

<p>Today we install the cli tool manually - pending is a container to deliver these more easily.</p>

<p>The CLI is named <code class="highlighter-rouge">ais</code>; it must be run on a Linux system.</p>

<h2 id="deploy-ais-using-helm">Deploy AIS using Helm</h2>

<h3 id="helm-install-wrapper">Helm Install Wrapper</h3>

<p>Deployment is automated (with all the above in place) via Helm install. The <code class="highlighter-rouge">values.yaml</code> in the chart has a number of generic defaults or values that can’t be known ahead of time (like disk names etc) and so it is usually a requirement to supply those values on the helm install cmdline. It’s generally easier to capture those site-unique settings in a wrapper script to helm install - saves repeating the cmdline runes every time you want to deploy. A future version of this wrapper should consume an rc file or similar, but for now you copy the sample and customize:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    $ cd deploy/prod/k8s/helm/ais
    $ cp run_ais_sample.sh start_ais
    $ vi start_ais
</code></pre></div></div>

<p>The values to confirm/change are highlighted in the script and discussed in comments. All of them we have dealt with and noted earlier in this document:</p>

<ul>
  <li>The cluster name; the default is demo, and it’s easier to leave it at that! Services, daemonsets, pods etc will all include this name
    <ul>
      <li>Note: you must deploy in the default namespace at this time (work in progress)</li>
      <li>Note: you can only deploy one AIS cluster within a k8s cluster (work in progress)</li>
    </ul>
  </li>
  <li>Container image locations and versions; associated pull secrets, if needed</li>
  <li>Target node mount paths for AIS disks</li>
  <li>Where monitoring data will be stored</li>
  <li>Optional cpu requests/limits</li>
  <li>Kubernetes cluster CIDR, as used in Kubespray (<code class="highlighter-rouge">kube_pods_subnet</code> in <code class="highlighter-rouge">group_vars/k8s-cluster/k8s-cluster.yml</code> in the Kubespray inventory); required if there are to be AIS storage clients outside the k8s cluster</li>
  <li>AIS gateway external IP (as per metalLB section above); if you leave this empty then metalLB will allocate and you can discover the IP address in use using <code class="highlighter-rouge">kubectl</code>.</li>
  <li>Hostport on target nodes that will be forwarded to target pods; the default is 51081 and there’s no reason to change it</li>
</ul>

<h3 id="deploy-ais">Deploy AIS</h3>

<h4 id="update-dependencies">Update Dependencies</h4>

<p>The <code class="highlighter-rouge">start_ais</code> script will pull these dependencies in if they are absent, but thereafter will not update dependencies - you choose when you want to update.</p>

<p>To update the Grafana and Graphite dependencies, run</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    $ cd deploy/prod/k8s/helm/ais/charts
    $ helm dependency update
</code></pre></div></div>

<p>You only need to do that once initially, then again later if you want to update those applications as part of an install.</p>

<h4 id="helm-install">Helm Install</h4>

<p>Now run the wrapper script we created above to start the AIS cluster:</p>

<p>$ cd deploy/prod/k8s/helm/ais</p>

<p>$ ./start_ais</p>

<h3 id="confirm-success">Confirm Success</h3>

<p>The first ever startup will take a little longer as all nodes retrieve the container image. Thereafter startup takes around 30s for all pods to be created and to form an AIS cluster. At that time, a</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ kubectl get pods --selector=app=ais
</code></pre></div></div>

<p>will show target and proxy pods as Ready. The AIS CLI will show cluster status:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>`$ env AIS_URL=http://&lt;ext-ip&gt;:51080 ais status`
</code></pre></div></div>

<p>If you configured external access, then the following should show an endpoint (and not   <none>):</none>
</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>`$ kubectl get ep demo-ais-gw`
</code></pre></div></div>

<h2 id="http-endpoint-for-clients-to-access">http Endpoint For Clients to Access</h2>

<p>External clients use <code class="highlighter-rouge">http://&lt;external-IP-or-dns-name&gt;:51080</code> for AIS access.</p>

<h2 id="grafana-access">Grafana Access</h2>

<p>This moves around on a nodePort service under the current AIS install - so the port changes if you helm delete and reinstall (not a common operation). To retrieve the current port in use:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    $ kubectl get service/demo-grafana
    NAME           TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
    demo-grafana   NodePort   10.233.13.178   &lt;none&gt;        80:30564/TCP   8m37s
</code></pre></div></div>

<p>In this example, port 30564 (on any node) can be contacted for Grafana.</p>

<p>If this is the first time Grafana has been installed, the generated password can be retrieved using:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    $ kubectl get secret --namespace default demo-grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo'
</code></pre></div></div>

<p>Alternatively, changed the password using:</p>

<p>```
$ kubectl exec -it demo-grafana -- grafana-cli admin reset-admin-password --homepath /usr/share/grafana “newpassword”</p>

<!-- Docs to Markdown version 1.0β18 -->

  </div><a class="u-url" href="/aistore/deploy/prod/k8s" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/aistore/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">AIStore - distributed object storage</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">AIStore - distributed object storage</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>AIStore is a lightweight object storage system with the capability to linearly scale-out with each added storage node and a special focus on petascale deep learning. See more at: github.com/NVIDIA/aistore
</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
