AIStore **4.0** is a major release that introduces a v2 object-metadata format and a chunked object representation (objects stored and managed as multiple chunks). This release also adds native multipart upload and a new [GetBatch API](#getbatch-api-ml-endpoint) for high-throughput retrieval of batches (objects and/or archived files).

In-cluster ETL has been extended with _ETL pipelines_, allowing users to chain transformations without intermediate buckets. Observability in 4.0 consolidates on Prometheus (as the sole monitoring backend) and adds disk-level capacity alerts

All subsystems, extensions, and modules were updated to support the new functionality. The CLI adds a cluster dashboard, improved feature-flag management, and numerous usability improvements. Configuration updates include a new `chunks` section and additional knobs to support clusters with hundreds of millions of objects and to improve runtime throttling.

This release arrives with nearly 300 commits since the previous [3.31](https://github.com/NVIDIA/aistore/releases/tag/v1.3.31) and maintains compatibility with the previous version, supporting rolling upgrades.

## Table of Contents

- [Object Metadata Version 2](#object-metadata-version-2)
- [Chunked Objects](#chunked-objects)
- [Native API: Multipart Upload (unified implementation)](#native-api-multipart-upload-unified-implementation)
- [GetBatch API (ML Endpoint)](#getbatch-api-ml-endpoint)
- [ETL](#etl)
- [Observability and Metrics](#observability-and-metrics)
- [Python SDK 1.16](#python-sdk-116)
- [CLI](#cli)
- [Configuration Changes](#configuration-changes)
- [Performance Optimizations](#performance-optimizations)
- [Build, CI/CD, and Testing](#build-cicd-and-testing)
- [Documentation](#documentation)

---

<a name="object-metadata-version-2"></a>
## Object Metadata Version 2

For the first time since AIStore's inception in 2018, the on-disk object metadata format has been upgraded.

Metadata v2 is now the default for all new writes, while v1 remains fully supported for backward compatibility. The system reads both versions seamlessly.

### What's New

The upgrade introduces persistent bucket identity and durable flags. Each object now stores its bucket ID (BID) alongside metadata. On load, AIStore checks the stored BID against the current bucket's BID from BMD (bucket metadata). A mismatch marks the object as defunct and evicts it from the cache, enforcing strong referential integrity between every object and the exact bucket generation it was written to.

The previous format had limited room for new features. Version 2 adds a dedicated 8-byte field for future capabilities (e.g., storage-class, compression, encryption, write-back). The system now returns explicit, typed errors (for example, `ErrLmetaCorrupted`) when metadata is damaged or unparseable, improving troubleshooting and debuggability.

Legacy flags (`filename-too-long`, `lom-is-chunked`) keep their original on-disk placement for backward compatibility, but we no longer carve bits out of the BID for new features.

The integrity model is now stronger: BID and flags are verified on load (before any object access). If the bucket is missing or has a different BID, loads fail with explicit errors (`ErrBckNotFound`, `ErrObjDefunct`). From the user's perspective, a defunct object effectively disappears, though it remains on disk until the next `space-cleanup`.

The serialized layout now packs BID and flags alongside checksum and other fields. The (future-proof) format remains compact.

No manual migration is required. Clusters continue to read v1 metadata while all new writes automatically persist v2.

### Commit Highlights

* [ae9ed1d2](https://github.com/NVIDIA/aistore/commit/ae9ed1d21): Introduce LOM (Local Object Metadata) v2 with BID persistence and durable flags
* [cdd3beef](https://github.com/NVIDIA/aistore/commit/cdd3beef): Enforce defunct object detection when BID mismatches current bucket
* [b543dab7](https://github.com/NVIDIA/aistore/commit/b543dab7): Extend serialized layout with new packed fields (LID, flags)
* [12ac88fe](https://github.com/NVIDIA/aistore/commit/12ac88fe): Update error handling and `unpack()` logic for v2 metadata

Another closely related and notable on-disk metadata change is chunked objects and chunk manifests - see [Chunked Objects](#chunked-objects).

---

<a name="chunked-objects"></a>
## Chunked Objects

Version 4.0 introduces chunked objects as a new persistent storage format. The previous limitation that required storing all objects (small or large) as single monoliths has been removed. For chunked objects, AIStore maintains a _chunk manifest_ that describes their content: chunk sizes, checksums, and ordering. The manifest itself is compressed and protected by a checksum.

Each in-progress [multipart upload](#native-api-multipart-upload-unified-implementation) creates a uniquely identified partial manifest, tied to the bucket, object name, and upload ID.

Multiple uploads can proceed in parallel, with each upload identified by its own manifest ID. Partial manifests are persistent, and the cluster automatically checkpoints them after every N chunks (configured via the "checkpoint_every" setting below):

```json
{
  "chunks": {
    "objsize_limit": 0,
    "chunk_size": "1GiB",
    "checkpoint_every": 4
  }
}
```

> The full set of chunk-related options is described in the [Configuration Changes](#configuration-changes) section.

At any given time, however, only one completed manifest exists for an object, serving as the current object version.

There is no limit — hard or soft — on the number of chunks. Chunks are sequentially numbered and distributed across mountpaths, while the manifest includes fields such as MD5 and ETag for S3 compatibility and reserved flags for future extensions like compression or encryption.

Interrupted or abandoned uploads are also cleaned up automatically. Any orphaned chunks or manifests are discovered and removed by the `space-cleanup` job, which runs on its own when a mountpath goes out of space, but can just as easily be invoked by an admin at any time:

```console
$ ais space-cleanup --help
```

For users and applications, chunked objects are fully transparent. Chunked and monolithic formats coexist side by side, and the same is true across both the S3-compatible and native APIs. Standard GETs, range reads, and archive reads all continue to work out of the box.

> **Note:** Throughout this document, we use the terms "archive" and "shard" interchangeably to refer to packaged collections of files (TAR, TGZ, ZIP, etc.).

### Commit Highlights

- [c4133361](https://github.com/NVIDIA/aistore/commit/c4133361fa): Introduce object chunks and chunk manifest (part one); add initial unit test
- [61a0ba6b](https://github.com/NVIDIA/aistore/commit/61a0ba6b52): LZ4-compress manifest with XXHash64 trailer; include per-chunk path; set max manifest size
- [05174b66](https://github.com/NVIDIA/aistore/commit/05174b6674): On-disk structure: manifest flags, per-chunk flags; move to `cos.Cksum`; unit test pass
- [2c2afc1a](https://github.com/NVIDIA/aistore/commit/2c2afc1a80): Deterministic serialization order; enforce uint16 limits; reset `completed` on error
- [54a03394](https://github.com/NVIDIA/aistore/commit/54a033948c): Refactor "none" checksum handling; cap per-entry metadata size
- [9eaaba8d](https://github.com/NVIDIA/aistore/commit/9eaaba8db2): Add `StoreCompleted`/`StorePartial`; atomic finalize via `CompleteUfest`; reader/reader-at; checksums/ETag helpers
- [a2866fc6](https://github.com/NVIDIA/aistore/commit/a2866fc68a): Manifest `LoadPartial`; HRW/ordering fixes; expand unit tests and scripted tests
- [445f6e35](https://github.com/NVIDIA/aistore/commit/445f6e357b): Revamp content resolution; register `ut` (manifest) CT; rename chunk CT to `ch`; remove hardcoded limits
- [9bad95b4](https://github.com/NVIDIA/aistore/commit/9bad95b4b0): Transition manifest from xattr to a content type
- [efbacca6](https://github.com/NVIDIA/aistore/commit/efbacca646): Refine manifest loading paths (completed vs partial)
- [984b7f25](https://github.com/NVIDIA/aistore/commit/984b7f25f3): S3 multipart integration with chunk manifest

---

<a name="native-api-multipart-upload-unified-implementation"></a>
## Native API: Multipart Upload (unified implementation)

AIStore **4.0** extends its native API to support multipart uploads (MPU) — a capability previously available only through the S3-compatible interface. The native flow now mirrors the (de-facto) S3 standard: **initiate => upload-part => complete (or abort)**.

The implementation unifies MPU handling across both native and S3 APIs, making it a core storage feature rather than a protocol-specific one.

Each upload gets a unique ID; multiple uploads can run in parallel against the same object. As parts arrive, AIStore records them as chunks and keeps a partial manifest on disk (checkpointed), so long uploads survive restarts without losing state.

Completion doesn't stitch bytes into one monolith anymore — it finalizes the manifest and promotes the object to the new chunked format (see [Chunked Objects](#chunked-objects)).

Completion rules follow S3 semantics with one clarification: AIStore requires all parts from 1 to N (no gaps) to finalize.

> One practical note from testing: some providers enforce a **minimum part size** (5MiB for S3 with the last part excluded).

Chunks (a.k.a., "parts") may arrive unordered, duplicates are tolerated, and the most recent copy of a given `partNumber` (chunk number) wins. Partial completion is rejected.

For S3 and compatible Cloud backends, the whole-object ETag is derived from the per-chunk MD5s, and full-object checksums are computed at finalize.

Range and archival reads behave as expected after completion (the reader streams across chunks).

Backends are handled uniformly. The S3 path stays compatible, and the target's S3-specific code has been thinned to formatting and XML while the common logic lives in AIStore's core.

> **Go** and **Python** developers can use the multipart upload workflow directly through:
>
> * [Go API (`api/mpt.go`)](https://github.com/NVIDIA/aistore/blob/main/api/mpt.go)
> * [Python SDK (`multipart_upload.py`)](https://github.com/NVIDIA/aistore/blob/main/python/aistore/sdk/obj/multipart_upload.py)
>
> Both expose the same set of operations — create, upload parts, complete, and abort — consistent with S3 semantics.

CLI was also extended with matching commands (see [Multipart Upload Commands](#multipart-upload-commands)) and end-to-end (e2e) tests.

Finally, garbage collection (as in: orphaned chunks, old partial manifests) is managed automatically by AIStore’s space-management subsystem.

### Commit Highlights

* [cb56d192](https://github.com/NVIDIA/aistore/commit/cb56d19268): Implement native multipart upload APIs; refactor core logic out of S3 path
* [73a72a65](https://github.com/NVIDIA/aistore/commit/73a72a6542): Consolidate MPU datapaths; cleanup/refactor (major)
* [984b7f25](https://github.com/NVIDIA/aistore/commit/984b7f25f3): S3 multipart + chunk manifest: disallow partial completion; allow unordered/duplicate parts; compute whole checksum/ETag
* [846bea6c](https://github.com/NVIDIA/aistore/commit/846bea6c81): Redefine checksumming (per-chunk, full, ETag); tighten range-read behavior post-finalize
* [8ee86551](https://github.com/NVIDIA/aistore/commit/8ee86551d7): Add `AbortMultipartUpload` API with comprehensive tests
* [675c5022](https://github.com/NVIDIA/aistore/commit/675c502238): CLI: implement MPU commands; e2e coverage
* [6753ed8e](https://github.com/NVIDIA/aistore/commit/6753ed8e7a): Support versioning for chunked objects via MPU
* [bfc26fe6](https://github.com/NVIDIA/aistore/commit/bfc26fe6a1): Add MPU support for remote AIS backend (`remais` cluster)
* [a8fbf546](https://github.com/NVIDIA/aistore/commit/a8fbf546ed): Tests: handle AWS minimum part size to avoid `EntityTooSmall`
* [25372cb3](https://github.com/NVIDIA/aistore/commit/25372cb395): Python: support multipart upload

---

<a name="getbatch-api-ml-endpoint"></a>
## GetBatch API (ML Endpoint)

Machine-learning training and inference workloads operate on batches of samples or data items. To simplify application code and accelerate large-scale random reads, we added `GetBatch` in 4.0.

Exposed via the new `/v1/ml` endpoint, the API returns a single ordered archive - TAR by default - containing the requested objects and/or archived files in the user-specified order.

![GetBatch Animation](https://raw.githubusercontent.com/NVIDIA/aistore/refs/heads/main/docs/images/get-batch-simple.gif)

A `GetBatch` request may include any number of items and span multiple buckets.

Supported archive formats for input and output are: .tar, .tgz/.tar.gz, .tar.lz4, and .zip. Clients may read files stored in any of these formats and request the combined result in any of the four (formats) as well. The default output format is always `.tar`.

Ordering is strict: ask for `A, B, C` - and the `GetBatch` result will contain `A`, then `B`, then `C`.

Two delivery modes are available. The **streaming** path starts sending as the resulting payload is assembled. The **multipart** path returns two parts: a small JSON header (`apc.MossOut`) with per-item status and sizes, followed by the archive payload.

From the caller's perspective, each request behaves like a regular synchronous GET, and you can read multiple batches in parallel.

Naming inside the archive is predictable. By default: `<Bucket>/<ObjName>`. With `OnlyObjName`: just `<ObjName>`. For archive members (a.k.a. archived files), we append the path: `<Bucket>/<ObjName>/<archpath>` (or `<ObjName>/<archpath>` with `OnlyObjName`).

> A "just `<archpath>`" variant is intentionally not supported to avoid ambiguity.

Resulting batches can include misses. If you set `ContinueOnErr`, missing entries don't fail the request; they appear under `__404__/…` with size 0 so downstream code stays simple.

In the multipart mode, reported [`MossOut.Size`](https://github.com/NVIDIA/aistore/blob/main/api/apc/ml.go) reflects the exact size of each entry in a batch.

Chunked objects (see [Chunked Objects](#chunked-objects) section) are fully supported; reads and archive extraction behave as expected inside a batch.

> Range reads (`start/length`) are not supported in 4.0. We'll consider adding them based on demand.

Under the hood, the datapath stays transparent: plain objects, archive members, and the new chunked objects all read the way one would expect.

### For Developers

* **Go API:** [`api/ml.go`](https://github.com/NVIDIA/aistore/blob/main/api/ml.go) — exposes `GetBatch` and related request/response structures.

* **PyTorch Dataset:** [`python/aistore/pytorch/batch_iter_dataset.py`](https://github.com/NVIDIA/aistore/blob/main/python/aistore/pytorch/batch_iter_dataset.py) — provides `AISBatchIterDataset`, an iterable PyTorch dataset built on top of the GetBatch API. It supports multi-worker loading, batching, streaming mode, and memory-efficient iteration for large-scale ML training.

### Commit Highlights

* [blog: faster data retrieval for ML workloads](https://aistore.nvidia.com/blog/2025/10/06/get-batch-sequential)
* [5223dadefb](https://github.com/NVIDIA/aistore/commit/5223dadefb): get-batch: add best-effort page-cache warming; introduce bounded work-pool
* [f3f8b89182](https://github.com/NVIDIA/aistore/commit/f3f8b89182): get-batch: remove Rx polling, add burst draining; micro-optimize
* [bb57c6c140](https://github.com/NVIDIA/aistore/commit/bb57c6c140): get-batch: add GFN support for archive reads; refactor datapath
* [dc7f1855c3](https://github.com/NVIDIA/aistore/commit/dc7f1855c3): get-batch: GFN-based recovery (part three); timeout handling
* [9daca65de0](https://github.com/NVIDIA/aistore/commit/9daca65de0): get-batch: ensure identical cluster-map version across targets
* [120ace86f7](https://github.com/NVIDIA/aistore/commit/120ace86f7): get-batch: fix send-archive cleanup; tolerate missing receives
* [56cc7e2fbf](https://github.com/NVIDIA/aistore/commit/56cc7e2fbf): tests: add "empty default bucket" permutations for get-batch

---

<a name="etl"></a>
## ETL

ETL now supports pipelines: chain independently deployed transformations into a single flow, reuse building blocks, and compose transforms without modifying existing buckets or objects.

Both the CLI (see [ETL Operations](#etl-operations)) and the Python SDK were updated to make this easy.

### CLI Example

```bash
$ ais object put data.txt ais://src/data.txt
$ ais etl init --name etl-transform-1 -f https://raw.githubusercontent.com/NVIDIA/ais-etl/main/transformers/hello_world/etl_spec.yaml
$ ais etl init --name etl-transform-2 -f https://raw.githubusercontent.com/NVIDIA/ais-etl/main/transformers/md5/etl_spec.yaml
$ ais etl object "etl-transform-1>>etl-transform-2" ais://src/data.txt - ed076287532e86365e841e92bfc50d8  # MD5 of "Hello World!"
```

### Python SDK Example

```python
etl_reverse = client.etl("etl-reverse")
@etl_reverse.init_class()
class ReverseETL(FastAPIServer):
    def transform(self, data: bytes, *_args) -> bytes:
        return data[::-1]

etl_upper_case = client.etl("etl-upper-case")
@etl_upper_case.init_class()
class UpperCaseETL(FastAPIServer):
    def transform(self, data: bytes, *_args) -> bytes:
        return data.upper()

two_stage = etl_upper_case >> etl_reverse

obj = client.bucket("ais", "src").object("data.txt")
obj.get_writer().put_content(b"Hello ais-etl!")
print(obj.get_reader(etl=ETLConfig(name=two_stage)).read_all())

# b'!LTE-SIA OLLEH'
```

Note that the pipeline syntax `a>>b` applies to both CLI and SDK examples. Python imports are omitted for brevity.

### Argument Type Auto-Detection

The `arg_type` setting is gone from user-facing APIs. Between the unified ETL webserver frameworks and direct filesystem access (FQN), the system now **auto-selects** how to feed objects to transformers.

You can still pass FQN per request (query param) when you need to, but in the common case there's nothing to configure.

### ETL Resource Limits

ETL containers accept standard Kubernetes resource requests/limits for predictable scheduling and guardrails under load:

```yaml
name: my-etl-transformer
runtime:
  image: aistorage/my_transformer:latest
resources:
  requests:
    memory: "256Mi"
    cpu: "500m"
  limits:
    memory: "512Mi"
    cpu: "1"
```

### Commit Highlights

* [6a5b93b8](https://github.com/NVIDIA/aistore/commit/6a5b93b82d2): add pipeline support via `>>`; update CLI examples; parsing in `etl.go`/`tcbtco.go`.
* [36cc1d77](https://github.com/NVIDIA/aistore/commit/36cc1d77a5b): remove `arg_type` from CLI & Python SDK; docs updated.
* [602d95b5](https://github.com/NVIDIA/aistore/commit/602d95b5686): remove user `arg_type`; default to **FQN**; introduce `QparamETLFQN`; don't forward it during direct-put chains.
* [62df9705](https://github.com/NVIDIA/aistore/commit/62df970532d): allow specifying FQN via query param per request.
* [ade06df2](https://github.com/NVIDIA/aistore/commit/ade06df26fa): support ETL pipelines (`>>`); add `etl_pipeline` arg to `transform`; add `QPARAM_ETL_PIPELINE`; add tests.
* [a47d3c42](https://github.com/NVIDIA/aistore/commit/a47d3c422a7): remove `arg_type` env; use query param to specify FQN per request.
* [599070b1](https://github.com/NVIDIA/aistore/commit/599070b11d3): implement ETL pipeline header processing in webservers.

---

<a name="observability-and-metrics"></a>
## Observability and Metrics

Prometheus is now the sole supported monitoring backend. StatsD support, which had been deprecated since v3.28, has been removed entirely, together with its build tags, deployment scripts, and example dashboards.

If you still rely on StatsD (for instance in [`aisloader`](https://github.com/NVIDIA/aistore/blob/main/docs/aisloader.md)), you will need to maintain it separately.

The `/metrics` endpoint is more resilient: scrapes continue even if some metrics fail. Prometheus now records self-metrics (`promhttp_*`) for scrapes — in-flight counts and latencies — making it easier to troubleshoot the monitoring subsystem.

Metric labeling has also been made more consistent. Per-bucket and per-operation labels are applied uniformly across all object operations, including multipart uploads, and can be enabled or disabled via [configuration](https://github.com/NVIDIA/aistore/blob/main/docs/feature_flags.md).

With detailed labels disabled, AIStore emits leaner time series to reduce Prometheus load; with them enabled, it provides the full per-bucket breakdowns.

> See [feature-flags](https://github.com/NVIDIA/aistore/blob/main/docs/feature_flags.md) for `Enable-Detailed-Prom-Metrics`.

Backend metrics always retain bucket context either way, so dashboards remain stable.

The "too many goroutines" condition is now tiered into a warning and a critical alert. These alerts are only raised if the condition persists, and are cleared once goroutine counts return to normal. This reduces noise from transient spikes while ensuring that sustained overload is visible.

Worker throttling respects the alert levels, backing off earlier on warning and more aggressively on critical.

> Several long-running batch jobs (such as `prefetch`, `etl`, `copy-objects`, `blob-download` and more) allow you to specify the number of concurrent workers. In 4.0, the system can now automatically throttle those jobs when a node is under sustained goroutine pressure. At the warning level, concurrency is reduced earlier; at the critical level, it is reduced more aggressively.

This helps prevent runaway parallelism from worsening overload conditions, without requiring operator intervention.

### Commit Highlights

* [ea84db1209](https://github.com/NVIDIA/aistore/commit/ea84db1209): observability: add high-num-goroutines yellow alert; amend throttle
* [53442fc48f](https://github.com/NVIDIA/aistore/commit/53442fc48f): ais: centralize prometheus variable labels; complete-mpt
* [7463eec355](https://github.com/NVIDIA/aistore/commit/7463eec355): remove StatsD support (fully deprecated)
* [49f783fcc0](https://github.com/NVIDIA/aistore/commit/49f783fcc0): prometheus: serve /metrics with continue-on-err; remove StatsD (part two)

---

<a name="python-sdk-116"></a>
## Python SDK 1.16

Version 1.16 of the Python SDK delivers improvements across the ETL framework, object operations, and the GetBatch API.

The release also strengthens retry handling, logging, and benchmarking support.

The [ETL framework](https://github.com/NVIDIA/aistore/tree/main/python/aistore/sdk/etl/webserver) is now more expressive. Users can chain together multiple ETLs into pipelines, making it possible to build end-to-end transformations without intermediate buckets.

Initialization has been simplified, with per-object argument evaluation replacing global configuration, and logging from ETL webservers is now standardized for easier monitoring.

Serialization helpers have also been consolidated into the [`ais-etl` runtime](https://github.com/NVIDIA/ais-etl/tree/main/runtime) for consistency.

On the **object API** side, the SDK now supports multipart upload workflows.

A dedicated interface allows objects to be uploaded in parts, completed, or aborted, matching the behavior of major cloud storage providers.

Single-object copy and transform operations have been extended with `latest` and `sync` options to improve consistency guarantees.

The [GetBatch API](#getbatch-api-ml-endpoint) has been extended to support `.zip` archives

Higher-level abstractions are also provided to simplify batch iteration and dataset handling, making it easier to integrate GetBatch into training and preprocessing pipelines.

Additional refinements include suppression of noisy SSL warnings (when certificate verification is intentionally disabled), a centralized retry manager that improves resilience under transient errors, and new benchmarking scripts for evaluating GetBatch performance.

Authentication flows also received minor fixes an polish.

### Commit Highlights

* [25372cb395](https://github.com/NVIDIA/aistore/commit/25372cb395): python/sdk: add multipart upload support for objects
* [cf8d9ef22c](https://github.com/NVIDIA/aistore/commit/cf8d9ef22c): python: add GET batch performance benchmark script
* [36cc1d77a5](https://github.com/NVIDIA/aistore/commit/36cc1d77a5): etl: remove `arg_type` config from CLI and Python SDK; update docs
* [62df970532](https://github.com/NVIDIA/aistore/commit/62df970532): etl/webserver: support specifying FQN via query param per request
* [a8fbf546ed](https://github.com/NVIDIA/aistore/commit/a8fbf546ed): tests: refactor AWS multipart uploads minimum part size handling

---

<a name="cli"></a>
## CLI

CLI changes reflect the system-level updates in this release — multipart workflows, chunked storage, and smarter space management, while maintaining the CLI's focus on operational simplicity and discoverability.

<a name="multipart-upload-commands"></a>
### Multipart Upload Commands

There's a new top-level [`ais mpu`](https://github.com/NVIDIA/aistore/blob/main/docs/cli/object.md#multipart-upload) command. This (command) enables uploads of large objects through a create => put-part => complete workflow. Parts (all or some) can be uploaded in parallel; multiple concurrent uploading sessions are also supported.

The command flow is straightforward:

* `mpu create` — starts a session and returns unique UPLOAD_ID
* `mpu put-part` — uploads individual parts (any order, fully parallelizable)
* `mpu complete` — makes a new version of the (chunked) object visible and accessible
* `mpu abort` — cancels and cleans up

Parts (or chunks) can be uploaded concurrently from different processes or machines, making it suitable for distributed upload scenarios and integration with S3-compatible tooling.

Both positional arguments and flag-based syntax are supported throughout, with `--verbose` flags providing progress tracking for long-running operations.

The workflow integrates with AIStore's existing bucket and object operations while maintaining compatibility with multipart upload semantics across backends.

<a name="etl-operations"></a>
### ETL Operations

The `ais etl` command set received ease-of-use type improvements for day-to-day operations. Users can now deploy multiple ETL jobs from a single YAML file using standard document separators (`---`), and override individual settings via CLI flags like `--name`, `--comm-type`, or `--object-timeout` without editing the spec.

Pipeline syntax integrates seamlessly into both object and bucket commands - just quote the chain: `"etl-1>>etl-2>>etl-3"`. For bucket transformations, you have fine-grained control over what gets processed (`--prefix`, `--template`, `--list`) and how (`--dry-run` for previews, `--num-workers` for concurrency, `--prepend` for output naming).

Operational commands now support bulk actions (`ais etl stop --all`, `ais etl rm --all`) and better diagnostics: `ais etl show errors` surfaces transformation failures, while `ais etl view-logs` lets you drill into specific target nodes. ETLs stuck in 'initializing' state can be terminated with a simple `stop` command.

<a name="feature-flags"></a>
### Feature Flags

[Feature flags](https://github.com/NVIDIA/aistore/blob/main/docs/feature_flags.md) now display in a 3-column format (FEATURE | TAGS | DESCRIPTION) with a comprehensive tagging system that makes trade-offs explicit.

Tags like `perf`, `overhead`, `security`, and domain identifiers (`s3`, `etl`, `mpu`) provide instant context. Integrity indicators —`integrity+` (enhanced safety), `integrity-` (performance-over-safety trade-off), `integrity?` (complex scenarios) — clarify the implications of enabling a flag.

The CLI prevents configuration conflicts through validation: mutually exclusive flags like `Disable-Cold-GET` and `Streaming-Cold-GET` are rejected.

Non-default currently-enabled features are visually highlighted in the full feature table, making it easy to audit cluster and bucket configurations at a glance. Features work at both cluster scope (`ais config cluster features`) and bucket scope (`ais bucket props`), with TAB completion throughout and simple reset via `ais bucket props set BUCKET features none`.

### Chunked Object Listing

Chunked objects added in 4.0 are now visible in listings via the `--chunked` flag. Running `ais ls --chunked` adds a `CHUNKED` column that shows "yes" for objects stored as chunks (typically from multipart uploads) and remains blank for regular monolithic objects. The flag integrates with other property displays, e.g.:

```console
$ ais ls ais://nnn --props size --chunked
NAME             SIZE            CHUNKED
README.md        11.46KiB
largefile        256.00MiB       yes

$ ais ls ais://nnn --props all --chunked
NAME         SIZE        CHECKSUM            ATIME                  VERSION COPIES  CUSTOM                                         LOCATION                                  CHUNKED   STATUS
README.md    11.46KiB    0868c3e1dce966f2    07 Oct 25 14:46 EDT    1       1       -                                              t[qBDt8081]:mp[/tmp/ais/mp2/1, nvme0n1]             ok
largefile    256.00MiB   8b727e68f38fea08    07 Oct 25 14:48 EDT    1       1       [ETag:"bce08b292bab67192272b538042e197f-18"]   t[qBDt8081]:mp[/tmp/ais/mp4/1, nvme0n1]   yes       ok
```

<a name="space-cleanup"></a>
### Space Cleanup

Space cleanup was fundamentally rewritten to handle the new chunked object storage model. The xaction now distinguishes between legitimate chunks (with valid manifests) and orphaned chunks from interrupted or abandoned multipart uploads, removing the latter after the grace period while preserving active sessions and finalized objects.

Beyond chunks, cleanup got smarter about data integrity: objects with missing metadata, size mismatches, or corrupted xattrs are now identified and removed (after the time window). The `--keep-misplaced` flag gives you control over HRW-misplaced objects, while `--rm-zero-size` handles the edge case of zero-byte files (opt-in only, since some workflows may intentionally create empty markers).

The grace period (`dont_cleanup_time`, default 2 hours) applies universally except for explicitly deleted content, which is cleaned aggressively. The system respects the `Keep-Unknown-FQN` feature flag for preserving unrecognized file patterns during forensics or debugging.

### Monitoring and Remote Cluster Operations

Job monitoring gained **column-based filtering** via the `--filter` flag, making it easier to find specific jobs in busy clusters. You can filter by any column using `"COLUMN=PATTERN"` syntax—for example, `--filter "STATE=Running"` shows only active jobs, `--filter "NODE=(node1|node2)"` limits results to specific nodes, and `--filter "BUCKET=.*ais-.*"` matches bucket patterns. Filters work with `--all` to include finished jobs and combine naturally with existing options like `--regex` and `--top`.

Remote cluster operations were reorganized under a new `ais show remote` command structure with three subcommands: `cluster` lists attached remote AIS clusters with their URLs and health, `config` inspects remote cluster configuration sections (with JSON output support), and `dashboard` provides at-a-glance performance and health metrics—throughput, load averages, disk usage, network status, running jobs, and cluster topology—all in a single view. The dashboard works for both local and remote clusters, and supports `--refresh` for continuous monitoring. The original `ais show remote-cluster` command remains available for backward compatibility.

### Commit Highlights

**Multipart Upload:**
* [675c5022](https://github.com/NVIDIA/aistore/commit/675c5022): implement multipart upload commands; e2e tests
* [742c33ef](https://github.com/NVIDIA/aistore/commit/742c33ef): implement multipart upload in chunks for large object PUT
* [6bc0109b](https://github.com/NVIDIA/aistore/commit/6bc0109b): fix string in completion message; enable `mpu` alias

**ETL:**
* [6a5b93b8](https://github.com/NVIDIA/aistore/commit/6a5b93b8): add support for ETL pipelines using `>>` operator
* [36cc1d77](https:/github.com/NVIDIA/aistore/commit/36cc1d77): remove `arg_type` config from CLI and python SDK; update docs

**Feature Flags:**
* [fe68589b](https://github.com/NVIDIA/aistore/commit/fe68589b): add feature flag tags; switch to 3-column view FEATURE | TAGS | DESCRIPTION
* [fe68589b](https://github.com/NVIDIA/aistore/commit/fe68589b): add feature flags validation logic for conflicts

**Chunked Objects:**
* [77a9c135](https://github.com/NVIDIA/aistore/commit/77a9c135): add `ais ls --chunked` command line; show CHUNKED column

**Space Cleanup:**
* [51a03f10](https://github.com/NVIDIA/aistore/commit/51a03f10): cleanup invalid FQNs; add Keep-Unknown-FQN feature
* [959e4d91](https://github.com/NVIDIA/aistore/commit/959e4d91): add bounded batch processing
* [0fdaed60](https://github.com/NVIDIA/aistore/commit/0fdaed60): unify x-flags; control misplaced cleanup

**Other CLI Improvements:**
* [25e2d1b7](https://github.com/NVIDIA/aistore/commit/25e2d1b7): amend version check and add 4.0 exception

**Job Monitoring:**
* [b73e7f72](https://github.com/NVIDIA/aistore/commit/b73e7f72): add column regex filter flag to `show job`

**Remote Cluster Operations:**
* [13a12b00](https://github.com/NVIDIA/aistore/commit/13a12b00): add `ais show remote cluster` and `dashboard` subcommands
* [179163b3](https://github.com/NVIDIA/aistore/commit/179163b3): add `ais show remote config`

---

<a name="configuration-changes"></a>
## Configuration Changes

### Storage Management and Cleanup

Space cleanup and LRU eviction now support **batch processing** to handle clusters with tens or hundreds of millions of objects. Previously, these xactions would accumulate lists of candidates for removal—old workfiles, misplaced objects, LRU candidates—and process them all at once, which became memory-prohibitive at scale.

The new `batch_size` knob (default 32K, range 1K-512K) applies to both `space` and `lru` configuration sections.
Cleanup joggers now process removals in bounded batches during filesystem walks, while LRU performs windowed eviction—selecting one batch per 4x window with any leftovers handled at the end. Each mountpath jogger maintains independent batch state, keeping memory usage predictable regardless of bucket size.

> See also [Space Cleanup](#space-cleanup) in the CLI section.

Grace periods were tightened to simplify edge-case handling. Both `space.dont_cleanup_time` and `lru.dont_evict_time` now enforce a minimum of **1 hour** (previously could be set arbitrarily low). The space cleanup default remains 2 hours. These windows protect recently-written content — including partial manifests from active multipart uploads — from premature removal during filesystem walks.

### Disk Utilization Smoothing

Disk utilization statistics now use **hybrid smoothing** to eliminate short-term spikes and improve throttling decisions. The new `disk.iostat_time_smooth` knob controls a linear history window over which utilization samples are age-weighted and blended with the current reading.

The algorithm computes a linear age-weighted average over the ring buffer (excluding the current sample), then blends it with the instantaneous value using an 80/20 mix. If current utilization exceeds the high watermark (or 90%), smoothing is bypassed entirely and the raw value is returned — ensuring the system throttles immediately under sustained load rather than waiting for the smoothed average to catch up.

When set to `0`, the system auto-scales the window from 1-4x `iostat_time_long` based on your polling interval. Setting a value smaller than `iostat_time_long` disables smoothing entirely, causing the system to report raw utilization values. The maximum allowed window is 10x `iostat_time_long`.

This smoothed utilization feeds into least-utilized-mountpath selection and capacity-based throttling throughout the datapath, making those decisions more stable under variable I/O patterns.

### Chunked Objects Configuration

A new `chunks` configuration section was added at both cluster and bucket scope in preparation for v4.1. Buckets inherit cluster settings by default but can override them individually.

> For usage context, see section [Chunked Objects](#chunked-objects).

**Configuration fields:**

```json
{
  "chunks": {
    "objsize_limit": 0,
    "chunk_size": "1GiB",
    "checkpoint_every": 4
  }
}
```

- **`objsize_limit`**: Reserved for v4.1. Will control the threshold for auto-chunking regular PUT operations. Currently must be set to `0` (auto-chunking disabled).

- **`chunk_size`**: Reserved for v4.1. Will specify the default chunk size for auto-chunked objects. Valid range is 1KiB to 5GiB, with a default of 1GiB.

- **`checkpoint_every`**: **Active in 4.0.** Controls how often partial manifests are persisted during multipart uploads. Setting this to `4` means the manifest is written to disk after every 4th chunk. A value of `0` keeps the manifest strictly in memory until completion (faster but not resilient to restarts). Range: 0-1024.

> **Note:** In 4.0, only `checkpoint_every` is functional. Multipart uploads (MPU) produce chunked storage using the client-specified part size, and the checkpoint interval controls manifest persistence. Auto-chunking of regular PUT operations will be enabled in v4.1 via the `objsize_limit` and `chunk_size` knobs.

The configuration structure is in place now to minimize or eliminate configuration changes in the v4.1 upgrade.

**Important:** Multipart uploads (MPU) always result in chunked storage using the client-specified part size, completely independent of `objsize_limit` and `chunk_size`. These settings only control automatic chunking of regular PUT operations.

Bucket-level overrides allow per-bucket tuning — for example, a bucket storing ML training checkpoints might use 2GiB chunks with infrequent checkpointing, while a bucket for streaming video content might use 256MiB chunks with aggressive checkpointing for better restart resilience.

### Commit Highlights

**Batch Processing:**
* [f726b1511](https://github.com/NVIDIA/aistore/commit/f726b1511): LRU: batch eviction w/ windowed selection
* [3b419b79e](https://github.com/NVIDIA/aistore/commit/3b419b79e): space: add bounded batch processing
* [4f3e7f0c3](https://github.com/NVIDIA/aistore/commit/4f3e7f0c3): add space knobs; move batch-size lru => space; resume MPU

**Disk Smoothing:**
* [cc8c9a976](https://github.com/NVIDIA/aistore/commit/cc8c9a976): ios: disk utilization smoothing

**Chunks Configuration:**
* [98bebdba3](https://github.com/NVIDIA/aistore/commit/98bebdba3): add 'chunks' config section (cluster scope)
* [742c33efc](https://github.com/NVIDIA/aistore/commit/742c33efc): add 'chunks' section (bucket scope / BMD)

---

<a name="performance-optimizations"></a>
## Performance Optimizations

AIStore 4.0 includes targeted optimizations to hot paths and frequently-accessed data structures. LOM (Local Object Metadata) packing was refined to reduce memory footprint and improve cache efficiency during high-throughput operations.

Content resolution — the internal mechanism for mapping object names to [mountpaths](https://github.com/NVIDIA/aistore/blob/main/docs/overview.md#mountpath) — was revamped to minimize string allocations and speed up lookups across mountpaths.

The `GetBatch` datapath shifted from polling-based receive loops to proactive burst draining (up to half channel capacity), eliminating unnecessary wait cycles during multi-object retrieval across entire cluster.

> To illustrate a minor implementation-level optimization: during "wait-for-receive" periods, a randomly selected target that assembles a combined cluster-wide output performs a best-effort lookahead for upcoming local reads to smooth the next items in large batches. This only helps when datasets far exceed available page cache. See related section: [GetBatch API (ML Endpoint)](#getbatch-api-ml-endpoint).

Periodic disk I/O stats collection now reads sysfs more efficiently by replacing buffered scanning with direct file reads.

While individually small, these changes compound across millions of operations per second in production workloads, reducing CPU overhead and improving responsiveness under load.

### Commit Highlights

* [6a48b16d6](https://github.com/NVIDIA/aistore/commit/6a48b16d6): core: micro-optimize LOM packing
* [445f6e357](https://github.com/NVIDIA/aistore/commit/445f6e357): revamp and optimize content resolution
* [f3f8b8918](https://github.com/NVIDIA/aistore/commit/f3f8b8918): get-batch: add burst draining; micro-optimize
* [813be30c1](https://github.com/NVIDIA/aistore/commit/813be30c1): ios: micro-optimize sysfs stats reading; fsync probe
* [d32c3125e](https://github.com/NVIDIA/aistore/commit/d32c3125e): micro-optimize mountpath => FS resolution

---

<a name="build-cicd-and-testing"></a>
## Build, CI/CD, and Testing

### Test Infrastructure for Chunked Objects

The test suite was substantially extended to exercise chunked objects (see [Chunked Objects](#chunked-objects)) across all existing integration tests.

The `ioContext` test helper now supports multipart uploads with configurable chunk counts and sizes, allowing any PUT-performing test to transparently switch between monolithic and chunked modes. This infrastructure change enabled validation of the core requirement: chunked objects must be functionally indistinguishable from monolithic ones across all operations—GETs, range reads, copies, transformations, and rebalancing.

Test coverage now includes concurrent multipart uploads, chunk cleanup on overwrite, remote AIS backend support, and stress testing with randomized file sizes (previously deterministic, now random across most tests to expose edge cases). Several tests required workarounds for direct filesystem access patterns that assumed monolithic storage—legitimate hacks that preserve the convenience of local playground testing while accommodating the new storage model.

### GitHub Actions and CI Pipelines

GitHub Actions workflows were updated across the board: all actions were bumped to their latest major versions (`actions/checkout@v5`, `actions/setup-go@v6`, `actions/setup-python@v6`). The `test-short` workflow gained comprehensive S3 testing support with s3cmd installation, configuration, and xxhash dependencies for checksum validation.

Python linting was separated into its own dedicated workflow, while the main lint workflow dropped the deprecated StatsD build tag and consolidated additional tags (`nethttp`, `debug`, `oteltracing`, `oci`, cloud providers) into a single pass. The `deploy-website` workflow now generates both API documentation and data models, with Netlify CLI pinned to v23.7.3 for stability. Build configurations removed StatsD compilation entirely, reducing the matrix from 8 to 7 build variants.

In GitLab, a new optional `test:short:chunks` pipeline validates chunked object behavior in isolation. Security scanning was integrated via SonarQube for static analysis. Because of the growing number of tests, pipeline timeouts were adjusted based on observed test durations: `test:short` increased from 40 to 45 minutes, `test:long` from 4 to 5 hours.

Linting was parallelized and separated between Python (moved to GitHub Actions) and Go (expanded to cover additional build tags). Dependencies were updated across both GitLab and GitHub workflows, including xxhash for checksumming and s3cmd for S3 backend tests.

### Test Coverage

New unit tests cover space cleanup scenarios (misplaced objects, workfiles, orphaned chunks, corrupted metadata), LRU eviction with recent configuration changes, and X509 certificate integration. ETL integration tests were expanded with download/transform workflows and SSL certificate handling. The get-batch API gained coverage for empty buckets, concurrent operations, and limited-coexistence scenarios.

### Build

OSS dependencies were upgraded across the board (except Kubernetes client libraries, which remain pinned for stability). The build system now lints additional build tags to catch platform-specific issues earlier.

### Commit Highlights

* [256a415cc](https://github.com/NVIDIA/aistore/commit/256a415cc): add `test:short:chunks` optional pipeline for chunked object testing
* [4eaab5511](https://github.com/NVIDIA/aistore/commiteaab5511): enhance `ioContext` for flexible file size range configuration
* [41fbf3be1](https://github.com/NVIDIA/aistore/commit/41fbf3be1): extend `ioContext` to support chunked and multipart uploads
* [ceb82bf06](https://github.com/NVIDIA/aistore/commit/ceb82bf06): space cleanup: add unit tests
* [32e7b0e71](https://github.com/NVIDIA/aistore/commit/32e7b0e71): space: add unit tests
* [f2a00004e](https://github.com/NVIDIA/aistore/commit/f2a00004e): parallelize lint; add separate GitHub lint action
* [95eed4910](https://github.com/NVIDIA/aistore/commit/95eed4910): upgrade all OSS packages except k8s client
* [2344dc9a5](https://github.com/NVIDIA/aistore/commit/2344dc9a5): add sonarqube integration (security)

---

<a name="documentation"></a>
## Documentation

In 4.0, we have added a few new technical blogs on assorted topics:

* **[The Perfect Line: Smooth Max Line Speed](https://aistore.nvidia.com/blog/2025/07/26/smooth-max-line-speed)** — shows AIStore sustaining 95% of theoretical network throughput (≈ 175 GiB/s) on a 16-node, 100 Gbps cluster.
* **[Single-Object Copy/Transform](https://aistore.nvidia.com/blog/2025/07/25/single-object-copy-transformation-capability)** — introduces a lightweight API for one-off inter-bucket transfers, comparing performance and usage patterns across CLI, Python SDK, and S3 clients.
* **[Hugging Face Integration](https://aistore.nvidia.com/blog/2025/08/22/huggingface-integration)** — illustrates dataset and model downloads from [Hugging Face](https://huggingface.co/) with batching, metadata parallelism, and private-repo authentication.
* **[Automated API Docs with GenDocs](https://aistore.nvidia.com/blog/2025/08/29/automated-api-documentation-generation-with-gendocs)** — explains the new annotation-driven tool that generates OpenAPI/Swagger specs directly from Go source.

Other documentation updates include:

* **Authentication (AuthN)** — clarified AuthN experimental status. Deployment now requires setting `AIS_AUTHN_SU_PASS` and `AIS_AUTHN_SECRET_KEY` before startup.
* **ETL guides** — added a section on Kubernetes resource limits (`resources.requests` / `resources.limits`) and removed the obsolete `arg_type` parameter.
* **Feature flags** — reorganized by domain (`s3`, `etl`, `mpu`) and impact (`perf`, `integrity±`), with a new three-column CLI view showing `FEATURE | TAGS | DESCRIPTION`. Validation now prevents conflicting settings.
* **Monitoring** — removed all StatsD references; Prometheus is now the only supported metrics backend (excepting `aisloader`). For details, see [Observability](#observability-and-metrics) section above.

### Commit Highlights

* [blog: The Perfect Line](https://aistore.nvidia.com/blog/2025/07/26/smooth-max-line-speed)
* [blog: single-object copy/transform](https://aistore.nvidia.com/blog/2025/07/25/single-object-copy-transformation-capability)
* [blog: HuggingFace integration](https://aistore.nvidia.com/blog/2025/08/22/huggingface-integration)
* [blog: GenDocs - automated API documentation](https://aistore.nvidia.com/blog/2025/08/29/automated-api-documentation-generation-with-gendocs)
* [c2644dc99](https://github.com/NVIDIA/aistore/commit/c2644dc99): docs: GenDocs tool for AIStore API documentation automation
* [179163b3c](https://github.com/NVIDIA/aistore/commit/179163b3c): docs: add security notice to AuthN; clarify password requirements
* [fe68589bd](https://github.com/NVIDIA/aistore/commit/fe68589bd): docs: add feature flag tags and 3-column view
* [7463eec35](https://github.com/NVIDIA/aistore/commit/7463eec35): docs: remove StatsD references
